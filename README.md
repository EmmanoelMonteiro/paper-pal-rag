# üìö Paper-Pal-RAG: Assistente de Leitura para Artigos Cient√≠ficos

---

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.9+-blue?style=for-the-badge&logo=python" alt="Python Version">
  <img src="https://img.shields.io/badge/LangChain-v0.2+-success?style=for-the-badge&logo=langchain" alt="LangChain Version">
  <img src="https://img.shields.io/badge/LM_Studio-Compatible-informational?style=for-the-badge&logo=ai" alt="LM Studio Compatible">
  <img src="https://img.shields.io/badge/ChromaDB-Local-important?style=for-the-badge" alt="ChromaDB">
</p>

## Vis√£o Geral do Projeto

O **Paper-Pal-RAG** √© um assistente de leitura inteligente em **Python** que usa **Retrieval Augmented Generation (RAG)** para ajudar voc√™ a extrair e entender informa√ß√µes de artigos cient√≠ficos.

Este projeto mostra uma arquitetura modular e escal√°vel, usando **LangChain** para orquestra√ß√£o, **LM Studio** como servidor da Large Language Model (LLM) local, e **ChromaDB** para armazenamento vetorial. √â perfeito para quem busca uma solu√ß√£o robusta e com arquitetura limpa para processar e consultar documentos.

## ‚ú® Funcionalidades

* **Ingest√£o de Documentos Flex√≠vel**: Carrega artigos cient√≠ficos em **PDF** (`PyPDFLoader`) e **TXT** (`TextLoader`).
* **Gera√ß√£o Aumentada por Recupera√ß√£o (RAG)**: O sistema busca informa√ß√µes relevantes nos seus documentos antes de gerar uma resposta com a LLM.
* **LLM Local com LM Studio**: Conecta-se facilmente a uma LLM rodando localmente via [LM Studio](https://lmstudio.ai/), garantindo privacidade e controle.
* **Armazenamento Vetorial Persistente**: Usa **ChromaDB** para armazenar e consultar embeddings de documentos de forma eficiente e persistente.
* **Interface de Chat Simples**: Interaja com o assistente por linha de comando, fazendo perguntas sobre o conte√∫do dos artigos.
* **Contexto Transparente**: Al√©m da resposta da LLM, o sistema mostra o **ID do chunk** e o **conte√∫do exato dos documentos** usados como base para a resposta, o que ajuda na valida√ß√£o e depura√ß√£o.
* **Arquitetura Limpa**: O c√≥digo √© organizado em camadas (`core`, `data`, `domain`, `infrastructure`, `presentation`) para promover modularidade, testabilidade e facilitar futuras expans√µes.

## üöÄ Como Executar o Projeto

Siga os passos abaixo para configurar e rodar o Paper-Pal-RAG no seu ambiente local.

### Pr√©-requisitos

* **Python 3.9+**: Verifique sua vers√£o com `python --version`.
* **LM Studio**: Baixe e instale o [LM Studio](https://lmstudio.ai/).

### 1. Clonar o Reposit√≥rio

```bash
git clone [https://github.com/seu-usuario/Paper-Pal-RAG.git](https://github.com/seu-usuario/Paper-Pal-RAG.git)
cd Paper-Pal-RAG
```

### 2. Configurar o Ambiente Virtual

√â **altamente recomendado** usar um ambiente virtual para gerenciar as depend√™ncias do projeto.

```bash
python -m venv .venv
```

### 3. Ativar o Ambiente Virtual
* Windows (PowerShell):

```bash
.\.venv\Scripts\activate
```

* Linux / macOS (Bash/Zsh):
```bash
source ./.venv/bin/activate
```

### 4. Instalar Depend√™ncias
Crie um arquivo chamado requirements.txt na raiz do projeto com o seguinte conte√∫do:

```
langchain
langchain-community
langchain-openai
pypdf
chromadb
python-dotenv
sentence-transformers
```
Agora, instale as depend√™ncias:
```bash
pip install -r requirements.txt
```

###5. Configurar o LM Studio
1. Abra o LM Studio.

2. Na barra lateral esquerda, navegue at√© a se√ß√£o **"Home"** (√≠cone de casa) ou **"Discover"** (√≠cone de lupa) para encontrar e baixar um modelo de sua prefer√™ncia (ex: `gemma-2b-it` ou um modelo da s√©rie Llama).

3. V√° para a aba **"Local Inference Server"** (geralmente a quarta aba, √≠cone de terminal com "localhost").

4. Clique em **"Start Server"**. Certifique-se de que o servidor est√° rodando na porta `1234` (esta √© a porta padr√£o e configurada no projeto).